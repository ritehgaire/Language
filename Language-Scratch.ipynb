{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "08b9261d-1768-460e-a235-c420323520a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='32768' class='' max='30252' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      108.32% [32768/30252 00:00<00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(#2) [Path('/Users/riteshgaire/.fastai/data/human_numbers/train.txt'),Path('/Users/riteshgaire/.fastai/data/human_numbers/valid.txt')]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from fastai.text.all import *\n",
    "path = untar_data(URLs.HUMAN_NUMBERS)\n",
    "path.ls()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b409f5aa-d13e-4fbe-aa00-d64221020a13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(#9998) ['one \\n','two \\n','three \\n','four \\n','five \\n','six \\n','seven \\n','eight \\n','nine \\n','ten \\n'...]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines = L()\n",
    "with open(path/'train.txt') as f: lines += L(*f.readlines()) \n",
    "with open(path/'valid.txt') as f: lines += L(*f.readlines()) \n",
    "lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1429ff8f-7de4-46d2-b690-60643cc5cbf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We take all those lines and concatenate them in one big stream. \n",
    "#To mark when we go from one number to the next, we use a . as a separator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0132998f-2c40-430e-aa7f-0750d5dc9367",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'one . two . three . four . five . six . seven . eight . nine . ten . eleven . twelve . thirteen . fo'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = ' . '.join([l.strip() for l in lines]) \n",
    "text[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f62aab43-ab21-48a9-9657-73bae05572e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['one', '.', 'two', '.', 'three', '.', 'four', '.', 'five', '.']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#We can tokenize this dataset by splitting on spaces:\n",
    "\n",
    "tokens = text.split(' ')\n",
    "tokens[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b9bd5691-3d71-4b08-9270-6da51a3f5af3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(#30) ['one','.','two','three','four','five','six','seven','eight','nine'...]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#To numericalize, we have to create a list of all the unique tokens (our vocab):\n",
    "\n",
    "vocab = L(*tokens).unique()\n",
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e46bb7fe-5f87-42e4-99cf-8d0f04ac1d4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(#63095) [0,1,2,1,3,1,4,1,5,1...]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Then we can convert our tokens into numbers by looking up the index of each in the vocab:\n",
    "\n",
    "word2idx = {w:i for i,w in enumerate(vocab)} \n",
    "nums = L(word2idx[i] for i in tokens)\n",
    "nums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "84b49b99-524c-44ff-bd32-ecac769ffb65",
   "metadata": {},
   "outputs": [],
   "source": [
    "#HERE GOES OUT FIRT MODEL FROM SCRATCH\n",
    "\n",
    "#One simple way to turn this into a neural network would be to specify that we are going to predict each word \n",
    "#based on the previous three words. We could create a list of every sequence of three words as our independent variables, \n",
    "#and the next word after each sequence as the dependent variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a6259e0f-93b2-4b1c-9bf5-05e1b65b7eae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(#21030) [(['one', '.', 'two'], '.'),(['.', 'three', '.'], 'four'),(['four', '.', 'five'], '.'),(['.', 'six', '.'], 'seven'),(['seven', '.', 'eight'], '.'),(['.', 'nine', '.'], 'ten'),(['ten', '.', 'eleven'], '.'),(['.', 'twelve', '.'], 'thirteen'),(['thirteen', '.', 'fourteen'], '.'),(['.', 'fifteen', '.'], 'sixteen')...]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#We can do that with plain Python. Let’s do it first with tokens just to confirm what it looks like:\n",
    "\n",
    "L((tokens[i:i+3], tokens[i+3]) for i in range(0,len(tokens)-4,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4dd5d6c0-b969-4fd7-b18e-72eb232991bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(#21031) [(tensor([0, 1, 2]), 1),(tensor([1, 3, 1]), 4),(tensor([4, 1, 5]), 1),(tensor([1, 6, 1]), 7),(tensor([7, 1, 8]), 1),(tensor([1, 9, 1]), 10),(tensor([10,  1, 11]), 1),(tensor([ 1, 12,  1]), 13),(tensor([13,  1, 14]), 1),(tensor([ 1, 15,  1]), 16)...]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Now we will do it with tensors of the numericalized values, which is what the model will actually use:\n",
    "\n",
    "seqs = L((tensor(nums[i:i+3]), nums[i+3]) for i in range(0,len(nums)-4,3)) \n",
    "seqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0fec6188-e749-4962-8211-56bd5e71233b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We can batch those easily using the DataLoader class. For now, we will split the sequences randomly:\n",
    "\n",
    "bs = 64\n",
    "cut = int(len(seqs) * 0.8)\n",
    "dls = DataLoaders.from_dsets(seqs[:cut], seqs[cut:], bs=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "665674c1-4daa-4fb9-8801-3542be021777",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We can now create a nural network that  takes three words as input, and returns a prediction of the probability\n",
    "#of possible nect words in the vocab\n",
    "# we will use three standard linear layers, nut with two tweaks\n",
    "\n",
    "#The first tweak is that the first linear layer will use only the first word’s embedding as activations, \n",
    "#the second layer will use the second word’s embedding plus the first lay‐ er’s output activations, \n",
    "#and the third layer will use the third word’s embedding plus the second layer’s output activations. \n",
    "#The key effect is that every word is interpreted in the information context of any words preceding it.\n",
    "\n",
    "#The second tweak is that each of these three layers will use the same weight matrix. \n",
    "#The way that one word impacts the activations from previous words should not change depending on the position of a word. \n",
    "#In other words, activation values will change as data moves through the layers, but the layer weights \n",
    "#themselves will not change from layer to layer. \n",
    "#So, a layer does not learn one sequence position; it must learn to handle all positions.\n",
    "#Since layer weights do not change, you might think of the sequential layers as “the same layer” repeated. \n",
    "#In fact, PyTorch makes this concrete; we can create just one layer and use it multiple times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c51ec7cd-6e18-40d7-8b82-8a511c7423b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Our Language Model in PyTorch\n",
    "\n",
    "class LMModel1(Module):\n",
    "    def __init__(self, vocab_sz, n_hidden):\n",
    "        self.i_h = nn.Embedding(vocab_sz, n_hidden)\n",
    "        self.h_h = nn.Linear(n_hidden, n_hidden)\n",
    "        self.h_o = nn.Linear(n_hidden,vocab_sz)\n",
    "    def forward(self, x):\n",
    "        h = F.relu(self.h_h(self.i_h(x[:,0]))) \n",
    "        h = h + self.i_h(x[:,1])\n",
    "        h = F.relu(self.h_h(h))\n",
    "        h = h + self.i_h(x[:,2])\n",
    "        h = F.relu(self.h_h(h))\n",
    "        return self.h_o(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e5dfa9a9-ffe1-4ccf-b2dd-4a8ab8980aaf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1.773262</td>\n",
       "      <td>1.845259</td>\n",
       "      <td>0.459948</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.401774</td>\n",
       "      <td>1.687664</td>\n",
       "      <td>0.464939</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.414476</td>\n",
       "      <td>1.652640</td>\n",
       "      <td>0.493226</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.375193</td>\n",
       "      <td>1.683244</td>\n",
       "      <td>0.418588</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Let’s try training this model and see how it goes:\n",
    "\n",
    "learn = Learner(dls, LMModel1(len(vocab), 64), loss_func=F.cross_entropy,\n",
    "                    metrics=accuracy)\n",
    "learn.fit_one_cycle(4, 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3b6bfc97-3976-4afb-8c67-687fcef3c665",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(29), 'thousand', 0.15165200855716662)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#To see if this is any good, let’s check what a very simple model would give us. \n",
    "#In this case, we could always predict the most common token, \n",
    "#so let’s find out which token is most often the target in our validation set:\n",
    "\n",
    "n,counts = 0,torch.zeros(len(vocab)) \n",
    "for x,y in dls.valid:\n",
    "    n += y.shape[0]\n",
    "    for i in range_of(vocab): counts[i] += (y==i).long().sum() \n",
    "idx = torch.argmax(counts)\n",
    "idx, vocab[idx.item()], counts[idx].item()/n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e8a40eea-6681-40fc-8461-6c5a5bbe57a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In above Code i\n",
    "#The most common token has the index 29, which corresponds to the token thousand. \n",
    "#Always predicting this token would give us an accuracy of roughly 15%, so we are faring way better!\n",
    "\n",
    "#This is a nice first baseline. Let’s see how we can refactor it with a loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f70f80cd-f3eb-4668-bfdc-6edc9d32a33e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our First Recurrent Neural Network\n",
    "#Looking at the code for our module, \n",
    "#we could simplify it by replacing the duplicated code that calls the layers with a for loop. \n",
    "#In addition to making our code simpler, \n",
    "#this will have the benefit that we will be able to apply our module equally well to token sequences of different \n",
    "#lengths—we won’t be restricted to token lists of length three:\n",
    "\n",
    "class LMModel2(Module):\n",
    "    def __init__(self, vocab_sz, n_hidden):\n",
    "        self.i_h = nn.Embedding(vocab_sz, n_hidden)\n",
    "        self.h_h = nn.Linear(n_hidden, n_hidden)\n",
    "        self.h_o = nn.Linear(n_hidden,vocab_sz)\n",
    "    def forward(self, x): \n",
    "        h=0\n",
    "        for i in range(3):\n",
    "            h = h + self.i_h(x[:,i]) \n",
    "            h = F.relu(self.h_h(h))\n",
    "        return self.h_o(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "27083525-18ca-4aae-b46e-8abdb9cba3af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1.795286</td>\n",
       "      <td>2.099795</td>\n",
       "      <td>0.464464</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.390736</td>\n",
       "      <td>1.880543</td>\n",
       "      <td>0.472308</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.403574</td>\n",
       "      <td>1.703555</td>\n",
       "      <td>0.496791</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.377218</td>\n",
       "      <td>1.734753</td>\n",
       "      <td>0.411695</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Let’s check that we get the same results using this refactoring:\n",
    "\n",
    "learn = Learner(dls, LMModel2(len(vocab), 64), loss_func=F.cross_entropy,\n",
    "                    metrics=accuracy)\n",
    "learn.fit_one_cycle(4, 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "03eaaec2-1c60-4783-950f-ac635b46a9d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#You will see that a set of activations is being updated each time through the loop, \n",
    "#stored in the variable h—this is called the hidden state (the activations that are updated at each step\n",
    "#of a recurrent neural network. \n",
    "#A neural network that is defined using a loop like this is called a recurrent neural net‐ work (RNN).\n",
    "#It is important to realize that an RNN is not a complicated new architec‐ ture, but simply a refactoring of a multilayer \n",
    "#neural network using a for loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "729e76d2-200b-4a17-8fbd-1a381eabaf8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Improving the RNN\n",
    "#Looking at the code for our RNN, one thing that seems problematic is that we are initializing our hidden state to zero \n",
    "#for every new input sequence. Why is that a prob‐ lem? We made our sample sequences short so they would fit easily \n",
    "#into batches. \n",
    "#But if we order those samples correctly, the sample sequences will be read in order by the model, \n",
    "#exposing the model to long stretches of the original sequence.\n",
    "#Another thing we can look at is having more signal: why predict only the fourth word when we could use the \n",
    "#intermediate predictions to also predict the second and third words? \n",
    "#Let’s see how we can implement those changes, starting with adding some state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6b25c6b6-4ba4-46d9-8ead-78acf0fbe0ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Maintaining the state of RNN\n",
    "#Because we initialize the model’s hidden state to zero for each new sample, \n",
    "#we are throwing away all the information we have about the sentences we have seen so far, \n",
    "#which means that our model doesn’t actually know where we are up to in the overall counting sequence. \n",
    "#This is easily fixed; we can simply move the initialization of the hidden state to __init__\n",
    "\n",
    "#But this fix will create its own subtle, but important, problem. \n",
    "#It effectively makes our neural network as deep as the entire number of tokens in our document. \n",
    "#For instance, if there were 10,000 tokens in our dataset, we would be creating a 10,000-layer neural network.\n",
    "\n",
    "#The problem with 10000 layer nn, when we get to the 10000th word dataset, we will still need to calculate the derivatives\n",
    "#all the way back to the first layer.\n",
    "#this will slow the process, and memory-intensive. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cd2373a-8a30-439e-9b93-e8a6ab879cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Chain Rule of Calculus: The backpropagation algorithm relies on the chain rule to calculate the derivatives of the \n",
    "#loss function with respect to the weights. Since each layer's output is a function of its input, which itself is a \n",
    "#function of the weights and outputs of previous layers, the derivative of the loss with respect to the weight in \n",
    "#any given layer depends on the derivatives of all the subsequent layers.\n",
    "\n",
    "#Dependency Chain: In a deep network, each layer's output depends on the layers before it. \n",
    "#Therefore, to understand how a change in the weight of the first layer affects the final loss, you need to consider \n",
    "#its impact on the second layer, then the impact of the second on the third, and so on, up to the last layer. \n",
    "#This creates a long chain of dependencies that must be resolved to update the weights correctly.\n",
    "\n",
    "##There are other rules as well: Gradient Descent: Neural networks typically use gradient descent or its variants \n",
    "#(like Adam, RMSprop, etc.) to optimize the loss function. \n",
    "#In gradient descent, the weights are updated in the opposite direction of the gradient of the loss function \n",
    "#with respect to the weights. This means calculating how a small change in each weight affects the loss, which is done \n",
    "#by taking derivatives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ab1da2cc-ff5e-49a7-9b59-6b57e952a58e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LMModel3(Module):\n",
    "    def __init__(self, vocab_sz, n_hidden):\n",
    "        self.i_h = nn.Embedding(vocab_sz, n_hidden)\n",
    "        self.h_h = nn.Linear(n_hidden, n_hidden)\n",
    "        self.h_o = nn.Linear(n_hidden,vocab_sz)\n",
    "        self.h = 0\n",
    "    def forward(self, x): \n",
    "        for i in range(3):\n",
    "            self.h = self.h + self.i_h(x[:,i])\n",
    "            self.h = F.relu(self.h_h(self.h)) \n",
    "            out = self.h_o(self.h)\n",
    "            self.h = self.h.detach()\n",
    "            return out\n",
    "    def reset(self): self.h = 0\n",
    "\n",
    "\n",
    "#we do not want to backpropagate the derivatives through the entire implicit neural network. \n",
    "#Instead, we will keep just the last three layers of gradients. To remove all of the gradient history in PyTorch, \n",
    "#we use the detach method.\n",
    "#Here is the new version of our RNN. It is now stateful, because it remembers its acti‐ vations between \n",
    "#different calls to forward, which represent its use for different sam‐ ples in the batch:\n",
    "\n",
    "\n",
    "#This model will have the same activations whatever sequence length we pick, because the hidden state will remember \n",
    "#the last activation from the previous batch. The only thing that will be different is the gradients computed at \n",
    "#each step: they will be calcula‐ ted on only sequence length tokens in the past, instead of the whole stream. \n",
    "#This approach is called backpropagation through time (BPTT)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07bc3990-8949-4fab-aa7a-3ac992ee53f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Jargon: Backpropagation Through Time\n",
    "#Treating a neural net with effectively one layer per time step (usu‐ ally refactored using a loop) \n",
    "#as one big model, and calculating gra‐ dients on it in the usual way. To avoid running out of memory and time, \n",
    "#we usually use truncated BPTT, which “detaches” the history of computation steps in the hidden state every few time steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "920c6fbe-d4f0-4da5-919b-97a6d34cdb98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(328, 64, 21031)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#To use LMModel3, we need to make sure the samples are going to be seen in a certain order.\n",
    "#First we divide the samples into m = len(dset)// bs group\n",
    "#(this is the equivalent of splitting the whole concaten‐ ated dataset into, \n",
    "#for example, 64 equally sized pieces, since we’re using bs=64 here)\n",
    "\n",
    "#m is the length of the each of this pieces\n",
    "\n",
    "m = len(seqs)//bs\n",
    "m,bs,len(seqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "08a1617e-c57d-4a7e-9f68-f6fc70daadd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The first batch will be composed of the samples\n",
    "   # (0, m, 2*m, ..., (bs-1)*m)\n",
    "#the second batch of the samples\n",
    "    #(1, m+1, 2*m+1, ..., (bs-1)*m+1)\n",
    "#and so forth. This way, at each epoch, the model will see a chunk of contiguous text of size 3*m \n",
    "#(since each text is of size 3) on each line of the batch.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1eecce0f-5499-4d48-a1c9-219428b75436",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The following function does the reindexing\n",
    "\n",
    "def group_chunks(ds, bs):\n",
    "    m = len(ds) // bs\n",
    "    new_ds = L()\n",
    "    for i in range(m): new_ds += L(ds[i + m*j] for j in range(bs)) \n",
    "    return new_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0c2413b6-6e60-4895-8d4d-25cc7cb55f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Then we just pass drop_last=True when building our DataLoaders to drop the last batch that does not have a shape of bs. \n",
    "#We also pass shuffle=False to make sure the texts are read in order:\n",
    "\n",
    "cut = int(len(seqs) * 0.8)\n",
    "dls = DataLoaders.from_dsets(\n",
    "    group_chunks(seqs[:cut], bs),\n",
    "    group_chunks(seqs[cut:], bs),\n",
    "    bs=bs, drop_last=True, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "336f76b7-b5c9-4a6e-b93f-3f1d00397029",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The last thing we do is a little treak of the training loop via a callback. it will call reset\n",
    "#method of our model at the begining of each epoch and before each validation phase. \n",
    "#since we implemented that method to set the hidden state to zero, this will make sure we start with a clean\n",
    "#state before reading those continuous chunks of text. \n",
    "#We can also start training a bit longer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a385e28c-fdad-43bb-9af2-3a253218e931",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>2.095834</td>\n",
       "      <td>2.121935</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.403627</td>\n",
       "      <td>1.818492</td>\n",
       "      <td>0.504567</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.210752</td>\n",
       "      <td>1.690260</td>\n",
       "      <td>0.528606</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.102061</td>\n",
       "      <td>1.716544</td>\n",
       "      <td>0.539904</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.017022</td>\n",
       "      <td>1.686464</td>\n",
       "      <td>0.533173</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.960161</td>\n",
       "      <td>1.670834</td>\n",
       "      <td>0.541587</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.911121</td>\n",
       "      <td>1.645974</td>\n",
       "      <td>0.564183</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.871230</td>\n",
       "      <td>1.637487</td>\n",
       "      <td>0.563702</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.850891</td>\n",
       "      <td>1.685664</td>\n",
       "      <td>0.557692</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.840873</td>\n",
       "      <td>1.661906</td>\n",
       "      <td>0.562500</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn = Learner(dls, LMModel3(len(vocab), 64), loss_func=F.cross_entropy,\n",
    "                    metrics=accuracy, cbs=ModelResetter)\n",
    "learn.fit_one_cycle(10, 3e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "363458c3-2c37-4277-8dd4-bf616a94115c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating more signals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "3971a193-0195-4bc7-a5da-7706a6f61b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "#It would be better if we predicted the next word after every single word, rather than every three words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "0227ddd6-aa28-44e6-bc55-2f5ec6c294e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#First change our data so that the dependent variable has each of the three next words after each of our three input words\n",
    "#instead of 3 we could use sl (for sequence length), and make it bigger. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "33146618-0942-4a09-8413-701e6702f2a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sl = 16\n",
    "seqs = L((tensor(nums[i:i+sl]), tensor(nums[i+1:i+sl+1]))\n",
    "         for i in range(0,len(nums)-sl-1,sl)) \n",
    "cut = int(len(seqs) * 0.8)\n",
    "dls = DataLoaders.from_dsets(group_chunks(seqs[:cut], bs),\n",
    "                             group_chunks(seqs[cut:], bs),\n",
    "                             bs=bs, drop_last=True, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "c1ee80b3-6563-4a7e-909e-5123036222fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(#16) ['one','.','two','.','three','.','four','.','five','.'...],\n",
       " (#16) ['.','two','.','three','.','four','.','five','.','six'...]]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Looking at the first element of seqs, we can see that it contains two lists of the same\n",
    "#size. The second list is the same as the first, but offset by one element:\n",
    "\n",
    "[L(vocab[o] for o in s) for s in seqs[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "71d2b664-80ec-4c60-ba3a-720e143fa270",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>3.263626</td>\n",
       "      <td>3.009837</td>\n",
       "      <td>0.220459</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.338349</td>\n",
       "      <td>1.894706</td>\n",
       "      <td>0.469238</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.745615</td>\n",
       "      <td>1.782273</td>\n",
       "      <td>0.475586</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.453624</td>\n",
       "      <td>1.724784</td>\n",
       "      <td>0.511475</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.278659</td>\n",
       "      <td>1.683755</td>\n",
       "      <td>0.527751</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.116195</td>\n",
       "      <td>1.670979</td>\n",
       "      <td>0.552002</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.971567</td>\n",
       "      <td>1.624209</td>\n",
       "      <td>0.554443</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.861114</td>\n",
       "      <td>1.665836</td>\n",
       "      <td>0.596924</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.775877</td>\n",
       "      <td>1.589161</td>\n",
       "      <td>0.626465</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.699975</td>\n",
       "      <td>1.738365</td>\n",
       "      <td>0.651042</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.649110</td>\n",
       "      <td>1.681382</td>\n",
       "      <td>0.641357</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.610694</td>\n",
       "      <td>1.840264</td>\n",
       "      <td>0.647054</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.577356</td>\n",
       "      <td>1.797722</td>\n",
       "      <td>0.672689</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.550578</td>\n",
       "      <td>1.731067</td>\n",
       "      <td>0.664795</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.537527</td>\n",
       "      <td>1.738897</td>\n",
       "      <td>0.659587</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Now we need to modify our model so that it outputs a prediction after every word, \n",
    "#rather than just at the end of a three-word sequence:\n",
    "\n",
    "class LMModel4(Module):\n",
    "    def __init__(self, vocab_sz, n_hidden):\n",
    "        self.i_h = nn.Embedding(vocab_sz, n_hidden)\n",
    "        self.h_h = nn.Linear(n_hidden, n_hidden)\n",
    "        self.h_o = nn.Linear(n_hidden,vocab_sz)\n",
    "        self.h = 0\n",
    "    def forward(self, x): \n",
    "        outs = []\n",
    "        for i in range(sl):\n",
    "            self.h = self.h + self.i_h(x[:,i]) \n",
    "            self.h = F.relu(self.h_h(self.h)) \n",
    "            outs.append(self.h_o(self.h))\n",
    "        self.h = self.h.detach()\n",
    "        return torch.stack(outs, dim=1)\n",
    "    def reset(self): self.h = 0\n",
    "\n",
    "#This model will return outputs of shape bs x sl x vocab_sz (since we stacked on dim=1). \n",
    "#Our targets are of shape bs x sl, so we need to flatten those before using them in F.cross_entropy:\n",
    "\n",
    "def loss_func(inp, targ):\n",
    "    return F.cross_entropy(inp.view(-1, len(vocab)), targ.view(-1))\n",
    "    \n",
    "#We can now use this loss function to train the model:\n",
    "\n",
    "learn = Learner(dls, LMModel4(len(vocab), 64), loss_func=loss_func,\n",
    "                metrics=accuracy, cbs=ModelResetter)\n",
    "learn.fit_one_cycle(15, 3e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "67b7ea66-3f97-4a1f-b978-711527c2bc12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>3.014019</td>\n",
       "      <td>2.611061</td>\n",
       "      <td>0.261230</td>\n",
       "      <td>00:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.144691</td>\n",
       "      <td>1.733702</td>\n",
       "      <td>0.471680</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.704649</td>\n",
       "      <td>1.885039</td>\n",
       "      <td>0.326335</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.495569</td>\n",
       "      <td>1.988733</td>\n",
       "      <td>0.369222</td>\n",
       "      <td>00:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.284528</td>\n",
       "      <td>1.973199</td>\n",
       "      <td>0.414958</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.129530</td>\n",
       "      <td>2.014080</td>\n",
       "      <td>0.400798</td>\n",
       "      <td>00:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.986453</td>\n",
       "      <td>1.937139</td>\n",
       "      <td>0.410807</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.861042</td>\n",
       "      <td>2.010647</td>\n",
       "      <td>0.409587</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.749368</td>\n",
       "      <td>2.107382</td>\n",
       "      <td>0.420573</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.659244</td>\n",
       "      <td>2.146489</td>\n",
       "      <td>0.429362</td>\n",
       "      <td>00:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.593598</td>\n",
       "      <td>2.297238</td>\n",
       "      <td>0.422933</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.545318</td>\n",
       "      <td>2.302000</td>\n",
       "      <td>0.425456</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.512262</td>\n",
       "      <td>2.402884</td>\n",
       "      <td>0.420166</td>\n",
       "      <td>00:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.491992</td>\n",
       "      <td>2.427224</td>\n",
       "      <td>0.416911</td>\n",
       "      <td>00:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.481013</td>\n",
       "      <td>2.420479</td>\n",
       "      <td>0.417318</td>\n",
       "      <td>00:04</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Multilayer RNNs\n",
    "\n",
    "class LMModel5(Module):\n",
    "    def __init__(self, vocab_sz, n_hidden, n_layers):\n",
    "        self.i_h = nn.Embedding(vocab_sz, n_hidden)  # Embedding layer\n",
    "        self.rnn = nn.RNN(n_hidden, n_hidden, n_layers, batch_first=True)  # RNN layer\n",
    "        self.h_o = nn.Linear(n_hidden, vocab_sz)  # Output linear layer\n",
    "        self.h = torch.zeros(n_layers, bs, n_hidden)  # Hidden state initialization\n",
    "\n",
    "    def forward(self, x):\n",
    "        res, h = self.rnn(self.i_h(x), self.h)  # Pass input and hidden state to the RNN\n",
    "        self.h = h.detach()  # Detach hidden state from the graph\n",
    "        return self.h_o(res)  # Apply the linear layer to the RNN output\n",
    "\n",
    "    def reset(self):\n",
    "        self.h.zero_()  # Reset hidden state to zero\n",
    "\n",
    "# Setup for training the model\n",
    "learn = Learner(dls, LMModel5(len(vocab), 64, 2),\n",
    "                loss_func=CrossEntropyLossFlat(),\n",
    "                metrics=accuracy, cbs=ModelResetter)\n",
    "learn.fit_one_cycle(15, 3e-3)  # Start training for 15 epochs with specified learning rate\n",
    "\n",
    "#deeper the model, longer the training time.\n",
    "\n",
    "#EACH EXTRA LAYER IS ANOTHER MATRIX MULTIPLICATION. \n",
    "\n",
    "\n",
    "#This is challenging because of what happens when you multiply by a matrix many times. \n",
    "#Think about what happens when you multiply by a number many times. For example, if you multiply by 2, \n",
    "#starting at 1, you get the sequence 1, 2, 4, 8,...and after 32 steps, you are already at 4,294,967,296. \n",
    "#A similar issue happens if you multiply by 0.5: you get 0.5, 0.25, 0.125...and after 32 steps, it’s 0.00000000023.\n",
    "\n",
    "#In practice, creating accurate models from this kind of RNN is difficult. \n",
    "#We will get better results if we call detach less often, and have more layers—this gives our RNN a longer time \n",
    "#j horizon to learn from and richer features to create.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "ea920fea-f940-4ccf-9af3-77242c27a2e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#AVOIDING EXPLOIDING ACTIVATIONS. \n",
    "\n",
    "#For RNNs, two types of layers are frequently used to avoid exploding activations: \n",
    "#gated recurrent units (GRUs) and long short-term memory (LSTM) layers.\n",
    "\n",
    "#oN LSTM THE CELL STATE IS RESPONSIBLE FOR KEEPING LONG SHORT TERM MEMORY, while the hidden state will focus on next token \n",
    "#to predict. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55425e15-41ff-435a-9ba7-2e58e7521dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#LSTM from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "f78be77b-96be-473f-b72b-1725e004c42a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#In an LSTM (Long Short-Term Memory) unit:\n",
    "\n",
    "#Forget Gate: A linear layer followed by a sigmoid activation determines which parts of the cell state to \n",
    "#retain or discard. Values near 0 indicate information to forget, while values near 1 suggest retaining information.\n",
    "\n",
    "#Input Gate and Cell Gate: The input gate decides which parts of the cell state to update (values near 1 update, \n",
    "#values near 0 do not). The cell gate, which also incorporates a tanh activation, \n",
    "#specifies the new values to be added to the cell state.\n",
    "\n",
    "#Output Gate: It selects which parts of the cell state contribute to the output. This involves passing the cell state through a tanh function, which is then modulated by the sigmoid output of the output gate to produce the new hidden state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "2093cc98-009b-4b21-a7bc-1a3c5381ce17",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMCell(Module):\n",
    "    def __init__(self, ni, nh):\n",
    "        self.forget_gate = nn.Linear(ni + nh, nh)\n",
    "        self.input_gate  = nn.Linear(ni + nh, nh)\n",
    "        self.cell_gate   = nn.Linear(ni + nh, nh)\n",
    "        self.output_gate = nn.Linear(ni + nh, nh)\n",
    "    def forward(self, input, state): \n",
    "        h,c = state\n",
    "        h = torch.stack([h, input], dim=1)\n",
    "        forget = torch.sigmoid(self.forget_gate(h)) \n",
    "        c = c * forget\n",
    "        inp = torch.sigmoid(self.input_gate(h)) \n",
    "        cell = torch.tanh(self.cell_gate(h))\n",
    "        c = c + inp * cell\n",
    "        out = torch.sigmoid(self.output_gate(h))\n",
    "        h = outgate * torch.tanh(c)\n",
    "        return h, (h,c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "5b88b4d6-0658-4af3-9147-76df76339657",
   "metadata": {},
   "outputs": [],
   "source": [
    "#it’s better to do one big matrix multiplication than four smaller ones\n",
    "# The optimized and refactored code then looks like this:\n",
    "\n",
    "class LSTMCell(Module):\n",
    "    def __init__(self, ni, nh):\n",
    "        self.ih = nn.Linear(ni,4*nh)\n",
    "        self.hh = nn.Linear(nh,4*nh)\n",
    "    def forward(self, input, state): \n",
    "        h,c = state\n",
    "            # One big multiplication for all the gates is better than 4 smaller ones\n",
    "        gates = (self.ih(input) + self.hh(h)).chunk(4, 1)\n",
    "        ingate,forgetgate,outgate = map(torch.sigmoid, gates[:3])\n",
    "        cellgate = gates[3].tanh()\n",
    "        c = (forgetgate*c) + (ingate*cellgate) \n",
    "        h = outgate * c.tanh()\n",
    "        return h, (h,c)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "b6e0bf04-d8d4-4027-958d-f51add27a56a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Here we use the PyTorch chunk method to split our tensor into four pieces. It works like this:\n",
    "\n",
    "t = torch.arange(0,10); t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "0fa3f489-4261-4e27-9e01-e43470b34960",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0, 1, 2, 3, 4]), tensor([5, 6, 7, 8, 9]))"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "  t.chunk(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "4f2022e6-0b3e-4e1a-bae0-f5c60d898d60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>3.023631</td>\n",
       "      <td>2.718089</td>\n",
       "      <td>0.288818</td>\n",
       "      <td>00:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.194491</td>\n",
       "      <td>1.895583</td>\n",
       "      <td>0.383138</td>\n",
       "      <td>00:08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.613236</td>\n",
       "      <td>1.815520</td>\n",
       "      <td>0.473877</td>\n",
       "      <td>00:08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.296872</td>\n",
       "      <td>2.099502</td>\n",
       "      <td>0.524007</td>\n",
       "      <td>00:09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.015141</td>\n",
       "      <td>2.115130</td>\n",
       "      <td>0.564941</td>\n",
       "      <td>00:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.737885</td>\n",
       "      <td>2.064255</td>\n",
       "      <td>0.593831</td>\n",
       "      <td>00:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.514384</td>\n",
       "      <td>2.217839</td>\n",
       "      <td>0.634440</td>\n",
       "      <td>00:08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.373000</td>\n",
       "      <td>1.980627</td>\n",
       "      <td>0.695231</td>\n",
       "      <td>00:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.269672</td>\n",
       "      <td>1.977432</td>\n",
       "      <td>0.708496</td>\n",
       "      <td>00:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.189167</td>\n",
       "      <td>1.934993</td>\n",
       "      <td>0.735433</td>\n",
       "      <td>00:05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.132983</td>\n",
       "      <td>1.977433</td>\n",
       "      <td>0.751953</td>\n",
       "      <td>00:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.092031</td>\n",
       "      <td>1.990011</td>\n",
       "      <td>0.759847</td>\n",
       "      <td>00:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.065401</td>\n",
       "      <td>1.951207</td>\n",
       "      <td>0.770589</td>\n",
       "      <td>00:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.050992</td>\n",
       "      <td>1.977391</td>\n",
       "      <td>0.766683</td>\n",
       "      <td>00:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.044317</td>\n",
       "      <td>1.980021</td>\n",
       "      <td>0.766683</td>\n",
       "      <td>00:05</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Training a Language Model Using LSTMs\n",
    "\n",
    "class LMModel6(Module):\n",
    "    def __init__(self, vocab_sz, n_hidden, n_layers):\n",
    "        self.i_h = nn.Embedding(vocab_sz, n_hidden)\n",
    "        self.rnn = nn.LSTM(n_hidden, n_hidden, n_layers, batch_first=True) \n",
    "        self.h_o = nn.Linear(n_hidden, vocab_sz)\n",
    "        self.h = [torch.zeros(n_layers, bs, n_hidden) for _ in range(2)]\n",
    "    def forward(self, x):\n",
    "        res,h = self.rnn(self.i_h(x), self.h) \n",
    "        self.h = [h_.detach() for h_ in h] \n",
    "        return self.h_o(res)\n",
    "    def reset(self):\n",
    "        for h in self.h: h.zero_()\n",
    "learn = Learner(dls, LMModel6(len(vocab), 64, 2),\n",
    "                    loss_func=CrossEntropyLossFlat(),\n",
    "                    metrics=accuracy, cbs=ModelResetter)\n",
    "learn.fit_one_cycle(15, 1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "74d104c2-2f3b-4b06-bdf5-3e26bb959b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dropout: The idea is randomly change some activations to zero at training time. This makes sure all\n",
    "#neurons actively work towards the output.\n",
    "\n",
    "class Dropout(Module):\n",
    "    def __init__(self, p): self.p = p \n",
    "    def forward(self, x):\n",
    "        if not self.training: return x\n",
    "        mask = x.new(*x.shape).bernoulli_(1-p) \n",
    "        return x * mask.div_(1-p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "079c2263-d6ce-4ad2-8ac5-fb6b89e77540",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The bernoulli_ method is creating a tensor of random zeros (with probability p) and ones (with probability 1-p), \n",
    "#which is then multiplied with our input before dividing by 1-p. Note the use of the training attribute, \n",
    "#which is available in any PyTorch nn.Module, and tells us if we are doing training or inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "a5c50e54-0c4b-435b-a2ed-b582ffba4bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Activation Regularization and Temporal Activation Regularization\n",
    "# Activation regularization (AR) and temporal activation regularization (TAR) are two regularization methods very \n",
    "#similar to weight decay. \n",
    "#When applying weight decay, we add a small penalty to the loss that aims at making the weights as small as possible. \n",
    "#For activation regularization, it’s the final activations produced by the LSTM that we will try to make as small as \n",
    "#possible, instead of the weights\n",
    "\n",
    "# loss += alpha * activations.pow(2).mean()\n",
    "\n",
    "# loss += beta * (activations[:,1:] - activations[:,:-1]).pow(2).mean()\n",
    "\n",
    "#alpha and beta are then two hyperparameters to tune. To make this work, we need our model with dropout to return \n",
    "#three things: the proper output, the activations of the LSTM pre-dropout, and the activations of the LSTM post-dropout.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "674b8e30-16cd-4532-a539-8dbb07a9af9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training a Weight-Tied Regularized LSTM\n",
    "\n",
    "#We can combine dropout (applied before we go into our output layer) with AR and TAR to train our previous LSTM. \n",
    "#We just need to return three things instead of one: the normal output of our LSTM, the dropped-out activations, \n",
    "#and the activations from our LSTMs. The last two will be picked up by the callback RNNRegularization for the \n",
    "#contributions it has to make to the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "2430e09c-986b-4035-90a2-d324fbcc04da",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Another useful trick we can add from the AWD-LSTM paper is weight tying. In a lan‐ guage model, \n",
    "#the input embeddings represent a mapping from English words to acti‐ vations, and the output hidden layer \n",
    "#represents a mapping from activations to English words. We might expect, intuitively, that these mappings \n",
    "#could be the same. We can represent this in PyTorch by assigning the same weight matrix to each of these layers:\n",
    "\n",
    "#self.h_o.weight = self.i_h.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "2bb55d19-22f8-4c2c-8a02-30bbd9caf166",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LMModel7(Module):\n",
    "    def __init__(self, vocab_sz, n_hidden, n_layers, p):\n",
    "        self.i_h = nn.Embedding(vocab_sz, n_hidden)\n",
    "        self.rnn = nn.LSTM(n_hidden, n_hidden, n_layers, batch_first=True) \n",
    "        self.drop = nn.Dropout(p)\n",
    "        self.h_o = nn.Linear(n_hidden, vocab_sz)\n",
    "        self.h_o.weight = self.i_h.weight\n",
    "        self.h = [torch.zeros(n_layers, bs, n_hidden) for _ in range(2)]\n",
    "    def forward(self, x):\n",
    "        raw,h = self.rnn(self.i_h(x), self.h) \n",
    "        out = self.drop(raw)\n",
    "        self.h = [h_.detach() for h_ in h] \n",
    "        return self.h_o(out),raw,out\n",
    "    def reset(self):\n",
    "        for h in self.h: h.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "d9a2797d-1d64-4206-9ff7-b584a5403e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We can create a regularized Learner using the RNNRegularizer callback:\n",
    "\n",
    "learn = Learner(dls, LMModel7(len(vocab), 64, 2, 0.5),\n",
    "                loss_func=CrossEntropyLossFlat(), metrics=accuracy,\n",
    "                cbs=[ModelResetter, RNNRegularizer(alpha=2, beta=1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "a7302647-88b2-468d-b791-9066604c5c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#A TextLearner automatically adds those two callbacks for us (with those values for\n",
    "#alpha and beta as defaults), so we can simplify the preceding line:\n",
    "\n",
    "learn = TextLearner(dls, LMModel7(len(vocab), 64, 2, 0.4),\n",
    "                    loss_func=CrossEntropyLossFlat(), metrics=accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "51096ed7-8bbe-46b7-9468-497cc5ebb641",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>2.595704</td>\n",
       "      <td>2.085654</td>\n",
       "      <td>0.468831</td>\n",
       "      <td>00:10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.555939</td>\n",
       "      <td>1.384637</td>\n",
       "      <td>0.664307</td>\n",
       "      <td>00:09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.802042</td>\n",
       "      <td>0.923289</td>\n",
       "      <td>0.792887</td>\n",
       "      <td>00:09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.391580</td>\n",
       "      <td>0.878531</td>\n",
       "      <td>0.820719</td>\n",
       "      <td>00:09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.201525</td>\n",
       "      <td>0.759407</td>\n",
       "      <td>0.848063</td>\n",
       "      <td>00:09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.110030</td>\n",
       "      <td>0.772179</td>\n",
       "      <td>0.846517</td>\n",
       "      <td>00:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.067381</td>\n",
       "      <td>0.807794</td>\n",
       "      <td>0.856364</td>\n",
       "      <td>00:10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.044814</td>\n",
       "      <td>0.705166</td>\n",
       "      <td>0.860921</td>\n",
       "      <td>00:08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.035101</td>\n",
       "      <td>0.691455</td>\n",
       "      <td>0.870931</td>\n",
       "      <td>00:10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.027616</td>\n",
       "      <td>0.773905</td>\n",
       "      <td>0.856608</td>\n",
       "      <td>00:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.022127</td>\n",
       "      <td>0.666365</td>\n",
       "      <td>0.879476</td>\n",
       "      <td>00:08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.018102</td>\n",
       "      <td>0.742781</td>\n",
       "      <td>0.870280</td>\n",
       "      <td>00:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.015070</td>\n",
       "      <td>0.733288</td>\n",
       "      <td>0.870280</td>\n",
       "      <td>00:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.013439</td>\n",
       "      <td>0.731563</td>\n",
       "      <td>0.868815</td>\n",
       "      <td>00:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.012191</td>\n",
       "      <td>0.731640</td>\n",
       "      <td>0.868896</td>\n",
       "      <td>00:09</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#We can then train the model, and add additional regularization by increasing the weight decay to 0.1:\n",
    "\n",
    "learn.fit_one_cycle(15, 1e-2, wd=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "b62d04d8-c48f-468e-823d-47520c1bb8cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 0., 0., 0., 0., 1., 1., 0., 1., 0.])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Create a tensor of zeros\n",
    "x = torch.zeros(10)\n",
    "\n",
    "# Apply bernoulli_ to randomly set some elements to 1 with a probability of 0.5\n",
    "x.bernoulli_(0.5)\n",
    "print(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a68f54cc-22fe-40e6-8813-d398994f7203",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
